#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç½‘ç«™åœ°å›¾(sitemap.xml)è‡ªåŠ¨ç”Ÿæˆè„šæœ¬
æ ¹æ®ç½‘ç«™æ–‡ä»¶è‡ªåŠ¨ç”ŸæˆSEOå‹å¥½çš„sitemap.xml
"""

import os
import re
import glob
import sys
import argparse
from datetime import datetime
from urllib.parse import quote

# ç½‘ç«™é…ç½®
WEBSITE_URL = "https://deepractice.ai"
SITEMAP_FILE = "../sitemap.xml"

# é¡µé¢ä¼˜å…ˆçº§å’Œæ›´æ–°é¢‘ç‡é…ç½®
PAGE_CONFIG = {
    # ä¸»è¦é¡µé¢
    "index.html": {"priority": 1.0, "changefreq": "weekly"},
    "blog.html": {"priority": 0.9, "changefreq": "weekly"},
    "prompts.html": {"priority": 0.9, "changefreq": "weekly"},
    
    # åšå®¢æ–‡ç« 
    "blog/*.html": {"priority": 0.8, "changefreq": "monthly"},
    
    # æ¼”ç¤ºé¡µé¢
    "presentation/*.html": {"priority": 0.7, "changefreq": "monthly"},
    
    # ä¸ªäººé¡µé¢
    "people/*.html": {"priority": 0.6, "changefreq": "monthly"},
    
    # æç¤ºè¯é¡µé¢
    "prompt-html/**/*.html": {"priority": 0.5, "changefreq": "monthly"},
    
    # é»˜è®¤é…ç½®
    "default": {"priority": 0.5, "changefreq": "monthly"}
}

def find_html_files(root_dir=".."):
    """æŸ¥æ‰¾æ‰€æœ‰HTMLæ–‡ä»¶"""
    html_files = []
    for ext in ["*.html", "*.htm"]:
        html_files.extend(glob.glob(f"{root_dir}/**/{ext}", recursive=True))
    
    # æ’é™¤ä¸éœ€è¦çš„æ–‡ä»¶
    excluded_patterns = [
        "/docs/",
        "/node_modules/", 
        "/.git/",
        "/temp/",
        "/backup/",
        "example.html",
        "test.html",
        "demo.html"
    ]
    
    filtered_files = []
    for file_path in html_files:
        should_exclude = False
        for pattern in excluded_patterns:
            if pattern in file_path:
                should_exclude = True
                break
        if not should_exclude:
            filtered_files.append(file_path)
    
    return filtered_files

def get_relative_path(file_path):
    """è·å–ç›¸å¯¹äºç½‘ç«™æ ¹ç›®å½•çš„è·¯å¾„"""
    # ç§»é™¤ "../" å‰ç¼€
    if file_path.startswith("../"):
        return file_path[3:]
    return file_path

def get_page_config(file_path):
    """æ ¹æ®æ–‡ä»¶è·¯å¾„è·å–é¡µé¢é…ç½®"""
    rel_path = get_relative_path(file_path)
    
    # æ£€æŸ¥ç²¾ç¡®åŒ¹é…
    if rel_path in PAGE_CONFIG:
        return PAGE_CONFIG[rel_path]
    
    # æ£€æŸ¥æ¨¡å¼åŒ¹é…
    for pattern, config in PAGE_CONFIG.items():
        if pattern == "default":
            continue
        
        # å°†globæ¨¡å¼è½¬æ¢ä¸ºæ­£åˆ™è¡¨è¾¾å¼
        if "*" in pattern:
            regex_pattern = pattern.replace("*", "[^/]*").replace("**", ".*")
            if re.match(f"^{regex_pattern}$", rel_path):
                return config
    
    # è¿”å›é»˜è®¤é…ç½®
    return PAGE_CONFIG["default"]

def get_last_modified(file_path):
    """è·å–æ–‡ä»¶çš„æœ€åä¿®æ”¹æ—¶é—´"""
    try:
        mtime = os.path.getmtime(file_path)
        return datetime.fromtimestamp(mtime).strftime('%Y-%m-%d')
    except:
        return datetime.now().strftime('%Y-%m-%d')

def generate_url(file_path):
    """ç”Ÿæˆå®Œæ•´çš„URL"""
    rel_path = get_relative_path(file_path)
    
    # å¤„ç†index.html - æ˜ å°„åˆ°æ ¹è·¯å¾„
    if rel_path == "index.html":
        return f"{WEBSITE_URL}/"
    
    # URLç¼–ç ï¼Œä½†ä¿ç•™è·¯å¾„åˆ†éš”ç¬¦
    url_path = quote(rel_path, safe='/.')
    return f"{WEBSITE_URL}/{url_path}"

def is_valid_html_page(file_path):
    """æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„HTMLé¡µé¢"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read(1000)  # åªè¯»å–å‰1000ä¸ªå­—ç¬¦
            
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åŸºæœ¬çš„HTMLç»“æ„
        if not re.search(r'<html[^>]*>', content, re.IGNORECASE):
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«titleæ ‡ç­¾ï¼ˆSEOé‡è¦ï¼‰
        if not re.search(r'<title[^>]*>.*?</title>', content, re.IGNORECASE | re.DOTALL):
            return False
            
        return True
    except:
        return False

def generate_sitemap(html_files, output_file):
    """ç”Ÿæˆsitemap.xmlæ–‡ä»¶"""
    
    # è¿‡æ»¤å‡ºæœ‰æ•ˆçš„HTMLé¡µé¢
    valid_pages = []
    for file_path in html_files:
        if is_valid_html_page(file_path):
            valid_pages.append(file_path)
    
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    valid_pages.sort(key=lambda x: get_page_config(x)["priority"], reverse=True)
    
    # ç”ŸæˆXMLå†…å®¹
    xml_content = ['<?xml version="1.0" encoding="UTF-8"?>']
    xml_content.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"')
    xml_content.append('        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"')
    xml_content.append('        xsi:schemaLocation="http://www.sitemaps.org/schemas/sitemap/0.9')
    xml_content.append('        http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd">')
    
    for file_path in valid_pages:
        config = get_page_config(file_path)
        url = generate_url(file_path)
        lastmod = get_last_modified(file_path)
        
        xml_content.append('    <url>')
        xml_content.append(f'        <loc>{url}</loc>')
        xml_content.append(f'        <lastmod>{lastmod}</lastmod>')
        xml_content.append(f'        <changefreq>{config["changefreq"]}</changefreq>')
        xml_content.append(f'        <priority>{config["priority"]}</priority>')
        xml_content.append('    </url>')
    
    xml_content.append('</urlset>')
    
    # å†™å…¥æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(xml_content))
    
    return len(valid_pages)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç½‘ç«™åœ°å›¾ç”Ÿæˆå·¥å…·')
    parser.add_argument('--output', '-o', default=SITEMAP_FILE, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--check', action='store_true', help='æ£€æŸ¥æ¨¡å¼ï¼ˆä¸ç”Ÿæˆæ–‡ä»¶ï¼‰')
    args = parser.parse_args()
    
    # æ£€æŸ¥æ˜¯å¦åœ¨æ­£ç¡®çš„ç›®å½•
    if not os.path.exists("generate_sitemap.py"):
        print("âŒ é”™è¯¯ï¼šè¯·åœ¨ .cloudflare ç›®å½•ä¸­è¿è¡Œæ­¤è„šæœ¬")
        sys.exit(1)
    
    print("ğŸ—ºï¸  ç½‘ç«™åœ°å›¾ç”Ÿæˆå™¨")
    print("=" * 50)
    
    # æŸ¥æ‰¾HTMLæ–‡ä»¶
    html_files = find_html_files()
    
    if not html_files:
        print("âŒ æœªæ‰¾åˆ°HTMLæ–‡ä»¶")
        sys.exit(1)
    
    print(f"ğŸ“„ æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
    
    if args.verbose:
        print("\nğŸ“‹ é¡µé¢é…ç½®:")
        for file_path in html_files:
            if is_valid_html_page(file_path):
                config = get_page_config(file_path)
                rel_path = get_relative_path(file_path)
                url = generate_url(file_path)
                print(f"  âœ… {rel_path}")
                print(f"     URL: {url}")
                print(f"     ä¼˜å…ˆçº§: {config['priority']}, æ›´æ–°é¢‘ç‡: {config['changefreq']}")
            else:
                print(f"  â­ï¸  è·³è¿‡: {get_relative_path(file_path)} (æ— æ•ˆHTML)")
    
    if args.check:
        print("\nâœ… æ£€æŸ¥å®Œæˆï¼ˆæœªç”Ÿæˆæ–‡ä»¶ï¼‰")
        return
    
    # ç”Ÿæˆsitemap
    try:
        page_count = generate_sitemap(html_files, args.output)
        print(f"\nâœ… æˆåŠŸç”Ÿæˆ {args.output}")
        print(f"ğŸ“Š åŒ…å« {page_count} ä¸ªæœ‰æ•ˆé¡µé¢")
        print(f"ğŸŒ ç½‘ç«™URL: {WEBSITE_URL}")
        print(f"ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æç¤ºä¸‹ä¸€æ­¥
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("   1. æ£€æŸ¥ç”Ÿæˆçš„sitemap.xmlæ–‡ä»¶")
        print("   2. æäº¤åˆ°Google Search Console")
        print("   3. æäº¤åˆ°ç™¾åº¦ç«™é•¿å¹³å°")
        print("   4. å¯ä»¥æ·»åŠ åˆ°robots.txtä¸­: Sitemap: https://deepractice.ai/sitemap.xml")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main() 